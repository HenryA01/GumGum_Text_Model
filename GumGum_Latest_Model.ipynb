{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5L2Q1JlERYx1"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "bvPLytK98xKp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Download stopwords if not already available\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faiWSGmg5duz"
   },
   "outputs": [],
   "source": [
    "#Load the Gold Data\n",
    "bbkpi_df = pd.read_csv('/content/bbkpi_gold.csv')\n",
    "ground_truth_cols = [c for c in bbkpi_df.columns[2:] if not c.endswith('_pred')]\n",
    "verity_prediction_cols = [c for c in bbkpi_df.columns[2:] if c.endswith('_pred')]\n",
    "\n",
    "bbkpi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3SWGAssQLWv"
   },
   "outputs": [],
   "source": [
    "# Load the Silver Data\n",
    "prod_silver_df = pd.read_excel('/content/prod_silver - fullyGPT2.0.xlsx')\n",
    "prod_silver_df = prod_silver_df[['TIMESTAMP', 'VIDEO_UUID', 'INTERVAL_ID', 'TEXT', 'LANGUAGE_CODE'] + ground_truth_cols]\n",
    "\n",
    "prod_silver_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FFLha3E2HNG"
   },
   "outputs": [],
   "source": [
    "print(prod_silver_df.shape)\n",
    "print(bbkpi_df.shape)\n",
    "\n",
    "print(prod_silver_df.columns)\n",
    "print(bbkpi_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EavX7Gs19__"
   },
   "outputs": [],
   "source": [
    "# Remove rows with NaN or empty text values\n",
    "prod_silver_df = prod_silver_df.dropna(subset=['TEXT'])  # Drop rows where TEXT is NaN\n",
    "prod_silver_df = prod_silver_df[prod_silver_df['TEXT'].str.len() > 0]  # Drop rows where TEXT is empty\n",
    "\n",
    "bbkpi_df = bbkpi_df.dropna(subset=['text'])  # Drop rows where text is NaN\n",
    "bbkpi_df = bbkpi_df[bbkpi_df['text'].str.len() > 0]  # Drop rows where text is empty\n",
    "\n",
    "print(prod_silver_df.shape)\n",
    "print(bbkpi_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIJvUaBl6NPj"
   },
   "outputs": [],
   "source": [
    "# Preparing silver data for training\n",
    "pre_train_df = pd.DataFrame({'text' : prod_silver_df['TEXT'].tolist(),\n",
    "                        'labels' : [x for x in prod_silver_df[ground_truth_cols].values.astype(float)]})\n",
    "pre_train_df = pre_train_df[pre_train_df['text'].apply(lambda s : isinstance(s, str) and len(s) >= 1)]\n",
    "\n",
    "print(len(pre_train_df))\n",
    "silver_dataset = Dataset.from_pandas(pre_train_df)\n",
    "pre_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77ArhWXARanj"
   },
   "outputs": [],
   "source": [
    "# Preparing gold data for test\n",
    "pre_test_df = pd.DataFrame({'text' : bbkpi_df['text'].tolist(),\n",
    "                        'labels' : [x for x in bbkpi_df[ground_truth_cols].values.astype(float)]})\n",
    "pre_test_df = pre_test_df[pre_test_df['text'].apply(lambda s : isinstance(s, str) and len(s) >= 1)]\n",
    "\n",
    "print(len(pre_test_df))\n",
    "gold_dataset = Dataset.from_pandas(pre_test_df)\n",
    "pre_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5q34oGTTcNYZ"
   },
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()                         # Convert to lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # Remove extra white spaces\n",
    "\n",
    "    tokens = text.split()                       # Tokenize and remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)                     # Join tokens back to a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJVzG-Z39GlP"
   },
   "outputs": [],
   "source": [
    "# Models and their tokenizers\n",
    "models = [\n",
    "    {'model': 'bert-base-uncased', 'tokenizer': 'bert-base-uncased'},\n",
    "    #{'model': 'roberta-base', 'tokenizer': 'roberta-base'},\n",
    "    #{'model': 'distilbert-base-uncased', 'tokenizer': 'distilbert-base-uncased'},\n",
    "\n",
    "\n",
    "    #{'model': 'microsoft/deberta-base', 'tokenizer': 'microsoft/deberta-base'},   #should use 1e-5 learning rate\n",
    "    #{'model': 'albert-base-v2', 'tokenizer': 'albert-base-v2'},\n",
    "    #{'model': 'xlnet-base-cased', 'tokenizer': 'xlnet-base-cased'}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Hyperparameter values to test\n",
    "batch_sizes = [16] #8, 16, 32\n",
    "epochs = [3] #3, 5\n",
    "learning_rates = [5e-5]  #1e-5, 5e-5\n",
    "weight_decays = [0.01] # 0, 0.1, 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAv9q_Gzk_xC"
   },
   "outputs": [],
   "source": [
    "for model_info in models:\n",
    "    model_name = model_info['model']\n",
    "    tokenizer_name = model_info['tokenizer']\n",
    "    print(f\"Training with {model_name} and {tokenizer_name}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # Tokenization function with preprocessing\n",
    "    def tokenize_function_with_preprocessing(examples):\n",
    "      # Apply the preprocess_text function\n",
    "      preprocessed_texts = [preprocess_text(text) for text in examples[\"text\"]]\n",
    "\n",
    "      # Tokenize using the tokenizer\n",
    "      return tokenizer(preprocessed_texts, padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "    # def tokenize_function(examples):\n",
    "    #   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "    # Apply preprocessing and tokenization to the dataset\n",
    "    tokenized_train_dataset = silver_dataset.map(tokenize_function_with_preprocessing, batched=True)                 #ADJUST - for training dataset, original is silver_dataset\n",
    "    test_dataset = gold_dataset.map(tokenize_function_with_preprocessing, batched=True)                            #ADJUST - for test dataset, original was gold_dataset\n",
    "\n",
    "\n",
    "    # Split train and validation datasets\n",
    "    # all_datasets = tokenized_train_dataset.train_test_split(test_size=0.2)      -----------------------------------------------\n",
    "    # train_dataset = all_datasets[\"train\"]\n",
    "    # val_dataset = all_datasets[\"test\"]\n",
    "\n",
    "\n",
    "    #Train on Silver, test on Gold\n",
    "    train_dataset = tokenized_train_dataset\n",
    "    val_dataset = test_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    id2label = dict((i, l) for i, l in enumerate(ground_truth_cols))\n",
    "    label2id = dict((l, i) for i, l in id2label.items())\n",
    "\n",
    "    # Loop over hyperparameter combinations\n",
    "    for batch_size, num_epochs, learning_rate, weight_decay in itertools.product(batch_sizes, epochs, learning_rates, weight_decays):\n",
    "        print(f\"\\nHyperparameters: batch_size={batch_size}, epochs={num_epochs}, learning_rate={learning_rate}, weight_decay={weight_decay}\")\n",
    "\n",
    "        # Initialize the model for each hyperparameter combination\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                                  problem_type=\"multi_label_classification\",\n",
    "                                                                  num_labels=len(id2label),\n",
    "                                                                  id2label=id2label,\n",
    "                                                                  label2id=label2id).to('cuda')\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            logging_dir='./logs',\n",
    "        )\n",
    "\n",
    "        def compute_multilabel_metrics(predictions, labels, threshold=0.5):\n",
    "            # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "            sigmoid = torch.nn.Sigmoid()\n",
    "            probs = sigmoid(torch.Tensor(predictions))\n",
    "            y_pred = np.zeros(probs.shape)\n",
    "            # next, use threshold to turn them into integer predictions\n",
    "            y_pred[np.where(probs >= threshold)] = 1\n",
    "\n",
    "            # finally, compute metrics\n",
    "            y_true = labels\n",
    "            f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "            f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            # return as dictionary\n",
    "            metrics = {'f1_micro': f1_micro_average,\n",
    "                      'f1_macro': f1_macro_average,\n",
    "                      'accuracy': accuracy}\n",
    "            return metrics\n",
    "\n",
    "        def compute_metrics(p):\n",
    "            preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "\n",
    "            result = compute_multilabel_metrics(\n",
    "                predictions=preds,\n",
    "                labels=p.label_ids)\n",
    "            return result\n",
    "\n",
    "\n",
    "        class MultilabelTrainer(Trainer):\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.get(\"logits\")\n",
    "                loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels.float())\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        print(f\"Training with {model_name}...\")\n",
    "\n",
    "        trainer = MultilabelTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate and store results\n",
    "        metrics = trainer.evaluate()\n",
    "\n",
    "\n",
    "        #ADJUST - CONFUSION MATRIX\n",
    "        # Generate predictions for the validation dataset\n",
    "        predictions = trainer.predict(val_dataset).predictions\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        probs = sigmoid(torch.Tensor(predictions))\n",
    "        y_pred = np.zeros(probs.shape)\n",
    "        y_pred[np.where(probs >= 0.5)] = 1\n",
    "        y_true = val_dataset['labels']\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        confusion_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        # Visualize confusion matrix for each label\n",
    "        for i, cm in enumerate(confusion_matrices):\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                        xticklabels=['Not ' + ground_truth_cols[i], ground_truth_cols[i]],\n",
    "                        yticklabels=['Not ' + ground_truth_cols[i], ground_truth_cols[i]])\n",
    "            plt.title(f'Confusion Matrix for {ground_truth_cols[i]}')\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Save the results\n",
    "        results.append({\n",
    "                'model': model_name,\n",
    "                'batch_size': batch_size,\n",
    "                'epochs': num_epochs,\n",
    "                'learning_rate': learning_rate,\n",
    "                'weight_decay': weight_decay,\n",
    "                'eval_accuracy': metrics['eval_accuracy'],\n",
    "                'eval_f1_macro': metrics['eval_f1_macro'],\n",
    "                'eval_f1_micro': metrics['eval_f1_micro'],\n",
    "                'eval_loss': metrics['eval_loss'],\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIkMav4rXrjU"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-fIZMbxlDfF"
   },
   "outputs": [],
   "source": [
    "# Print and compare results\n",
    "print(\"\\nAll Results:\")\n",
    "for result in results:\n",
    "    print(f\"Model: {result['model']}, Batch Size: {result['batch_size']}, Epochs: {result['epochs']}, \"\n",
    "          f\"Learning Rate: {result['learning_rate']}, Weight Decay: {result['weight_decay']}, \"\n",
    "          f\"F1 Macro: {result['eval_f1_macro']}, Accuracy: {result['eval_accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mUTSh3Kq3Wf"
   },
   "outputs": [],
   "source": [
    "# Optionally, save the results to a CSV file\n",
    "# df_results = pd.DataFrame(results)\n",
    "# df_results.to_csv(\"hyperparameter_results_learningrate_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlJvpXfcvUTW"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
