{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "s4h0-BZprcNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoHv1ERYscrm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import openai\n",
        "import json\n",
        "import re\n",
        "from IPython.display import display\n",
        "import sys\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Prompt the user to enter their API key\n",
        "openai.api_key = input(\"Please enter your OpenAI API key: \").strip()\n",
        "if not openai.api_key:\n",
        "    raise ValueError(\"OpenAI API key cannot be empty. Please enter a valid API key.\")\n",
        "\n",
        "\n",
        "def setup_openai():\n",
        "    client = openai.OpenAI()\n",
        "    return client\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(3),\n",
        "    wait=wait_exponential(multiplier=1, min=4, max=10)\n",
        ")\n",
        "\n",
        "def analyze_text(client, text):\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following text for content safety, assessing against these specific categories. Use the criteria provided to ensure accurate classification.\n",
        "\n",
        "    Text to analyze: {text}\n",
        "\n",
        "    Criteria for each category:\n",
        "    1. **Obscenity and Profanity**:\n",
        "      - **Explicit Profanity**: Flag strong, explicit profanities that are widely offensive across most contexts. This includes words that have explicit sexual connotations or are commonly used as vulgar insults (e.g., \"pussy,\" \"fuck,\" \"shit,\" \"cunt\"). The focus is on words that are inherently vulgar or offensive.\n",
        "      - **Moderate Profanity**: Flag moderate profanities if used in a clearly offensive or aggressive way (e.g., directed at someone or used as an insult). Include terms that may have a dual meaning if used offensively, such as \"bitch,\" \"ass,\" or \"damn\" when intended to insult or degrade.\n",
        "      - **Exclusions**: Ignore mild expressions (e.g., \"darn,\" \"shoot\") or language commonly accepted in polite conversation unless used in an intentionally derogatory way.\n",
        "      - **Offensive Gestures**: Include gestures or descriptions intended to shock, offend, or provoke others. Consider words and phrases that describe obscene gestures.\n",
        "      - **Graphic Content**: Flag only if content is clearly intended to shock, disgust, or offend. Exclude factual descriptions such as medical or clinical terms, which are used for educational or informative purposes.\n",
        "      - **Context**: Flag explicit language used in a serious, offensive, or derogatory manner, even if it may be intended humorously. Specifically flag words that are inherently offensive (e.g., \"shit,\" \"pussy\") regardless of context if they could be reasonably seen as offensive.\n",
        "      - **Examples to Flag**: Strong profanity (e.g., \"shit,\" \"fuck,\" \"cunt,\" \"pussy\", \"dick\"), vulgar gestures, sexual profanity, offensive anatomical references, and words like \"bitch\" or \"asshole\" when used as insults.\n",
        "      - **Examples to Exclude**: Mild exclamations (e.g., \"heck\"), medical terms (e.g., \"vagina\" in a clinical context), polite or non-offensive language used in a clearly non-offensive context.\n",
        "\n",
        "    2. **Death, Injury, or Military Conflict**:\n",
        "      - **Direct References**: Flag explicit mentions of human death, physical injury, or military combat that indicate realistic harm, violence, or direct threats.\n",
        "      - **Violence**: Include both realistic and descriptive accounts of physical harm as well as **statements that imply a threat, aggression, or an immediate risk of injury or violence against humans**, even if not explicitly detailed. This includes vague or aggressive language implying physical harm.\n",
        "      - **Military Conflict**: Flag literal mentions of military operations or combat involving humans.\n",
        "      - **Exclusions**: Ignore figurative, symbolic, or hyperbolic language (e.g., \"die of embarrassment,\" \"killing it\"), **religious or ceremonial contexts involving animal sacrifice**, casual references without actual intent (e.g., repeated phrases like \"die\" in a non-literal context), and **animal death** unless directly related to human violence.\n",
        "      - **Context**: Only flag if the mention is clearly literal, serious, and realistic. Disregard casual, humorous, figurative, symbolic, or ceremonial uses of words like \"die,\" \"kill,\" or similar terms that do not imply actual violence or harm to humans.\n",
        "      - **Examples to Flag**: Realistic descriptions of human death, serious injury, corpses, torture, blood, gore, explicit acts of violence, and scenes involving military combat.\n",
        "      - **Examples to Exclude**: Figurative expressions (e.g., \"dead tired,\" \"die of laughter\"), **religious or ceremonial contexts involving symbolic or ritual sacrifice** (e.g., \"the lamb must die as part of the ritual\"), casual or non-serious contexts (e.g., playful banter, jokes), hyperbole, and unrelated animal death.\n",
        "\n",
        "    3. **Arms and Ammunition**:\n",
        "      - **Explicit Mentions of Weapons**: Flag references to firearms, explosives, knives, and ammunition.\n",
        "      - **Military Equipment**: Include mentions of combat-specific items (e.g., tanks, missiles, grenades).\n",
        "      - **Indirect References**: Flag terms like \"loaded\" or \"armed\" when used in dangerous or combat-related contexts.\n",
        "      - **Exclusions**: Ignore generic military terms without specific weapons, metaphorical uses (e.g., \"armed with knowledge\"), and historical/toy weapons.\n",
        "      - **Context**: Only flag literal, serious references; disregard casual or non-combat uses.\n",
        "      - **Examples to Flag**: Firearms, military-grade weapons, tactical combat gear, improvised explosives.\n",
        "      - **Examples to Exclude**: Historical weapons (e.g., swords), non-combat terms, figurative phrases.\n",
        "\n",
        "    4. **Debated Sensitive Social Issues**:\n",
        "      - **Topics to Flag**: Active political conflicts, civil unrest, religious disputes, social justice controversies, or discriminatory policies.\n",
        "      - **Exclusions**: Ignore neutral or factual reporting, historical events, and academic discussions.\n",
        "      - **Context**: Flag if the topic could be seen as divisive or likely to incite debate.\n",
        "      - **Examples to Flag**: Current social justice issues, discriminatory policies, content on active conflicts.\n",
        "      - **Examples to Exclude**: Historical references, objective news reporting, unbiased commentary.\n",
        "\n",
        "    5. **Hate Speech & Acts of Aggression**:\n",
        "      - **Discriminatory Language**: Flag language targeting protected traits like race, ethnicity, religion, gender, etc. **Include gender-specific insults** that demean based on perceived sexual behavior (e.g., \"slut,\" \"whore\").\n",
        "      - **Derogatory Terms and Slurs**: Include slurs or demeaning terms intended to insult, belittle, or dehumanize.\n",
        "      - **Incitement to Violence**: Flag language encouraging harm or physical violence, including encouraging others to harm someone.\n",
        "      - **Intimidation or Threats**: Flag statements meant to instill fear, imply harm, or threaten violence against others. Include **explicit threats of physical harm**, even if not identity-based.\n",
        "      - **Exclusions**: Avoid casual disagreements, valid criticism, or non-hostile uses (e.g., \"destroyed in debate\" unless identity-based). However, **explicit personal threats** should not be excluded even if they seem exaggerated or part of a narrative.\n",
        "      - **Context**: Only flag if language is used aggressively or with harmful intent. **Direct threats** should be flagged even if the context is ambiguous or exaggerated.\n",
        "      - **Examples to Flag**: Slurs targeting identity traits, explicit bias or derogatory remarks, threats, intimidation statements, **explicit personal threats of harm** (e.g., \"I'm gonna blow your head off\").\n",
        "      - **Examples to Exclude**: Non-hostile disagreements, valid critiques, figurative non-hostile language.\n",
        "\n",
        "    6. **Adult & Explicit Sexual Content**:\n",
        "      - **Explicit Sexual References**: Flag sexual descriptions, adult themes, or explicit acts.\n",
        "      - **Erotic or Pornographic Content**: Include material intended to arouse or that depicts sexual situations.\n",
        "      - **Nudity and Suggestive Content**: Flag nudity, detailed anatomical descriptions, or sexual solicitation.\n",
        "      - **Exclusions**: Ignore medical or educational content and non-explicit artistic references.\n",
        "      - **Context**: Only flag if explicitly sexual or adult-oriented.\n",
        "      - **Examples to Flag**: Explicit sexual acts, erotic content, pornographic material, explicit nudity.\n",
        "      - **Examples to Exclude**: Medical information, sex education, non-explicit artistic references.\n",
        "\n",
        "    7. **Crime & Harmful Acts**:\n",
        "      - **Criminal Activities**: Flag illegal actions (e.g., theft, fraud) or association with law enforcement if relevant to harm.\n",
        "      - **Harmful Behavior**: Include threats of violence or aggressive intent.\n",
        "      - **Methods and Instructions**: Flag criminal methods or ongoing criminal acts.\n",
        "      - **Exclusions**: Ignore fictional or historical crime mentions unless tied to modern harm, and metaphorical language (e.g., “crime against fashion”).\n",
        "      - **Context**: Only flag if threats or criminal acts imply real, immediate harm.\n",
        "      - **Examples to Flag**: Criminal activities, harmful behavior, threats to public safety.\n",
        "      - **Examples to Exclude**: Historical crime mentions, metaphorical uses without harm.\n",
        "\n",
        "    8. **Illegal Drugs/Tobacco/Vaping**:\n",
        "      - **Illegal Drug Use or Trafficking**: Flag references to illegal drug use, dealing, or paraphernalia.\n",
        "      - **Tobacco and Vaping**: Include smoking or vaping references.\n",
        "      - **Alcohol Abuse**: Flag mentions of alcohol abuse, dependency, or alcoholism, including references such as \"alcoholic\" that imply problematic use.\n",
        "      - **Exclusions**: Ignore legal mentions, such as prescriptions or moderate social drinking.\n",
        "      - **Context**: Flag if there is dependency or problematic use, even without explicit details.\n",
        "      - **Examples to Flag**: Illegal drug use, excessive alcohol use, references to alcoholism, tobacco or vaping behaviors.\n",
        "      - **Examples to Exclude**: Legal prescriptions, moderate social drinking.\n",
        "\n",
        "    Return only a JSON dictionary in this exact format:\n",
        "    {{\n",
        "        \"Obscenity\": false,\n",
        "        \"Death\": false,\n",
        "        \"Arms\": false,\n",
        "        \"SensitiveIssues\": false,\n",
        "        \"HateSpeech\": false,\n",
        "        \"Adult\": false,\n",
        "        \"Crime\": false,\n",
        "        \"Drugs\": false\n",
        "    }}\n",
        "\n",
        "    Important context rules:\n",
        "    - Evaluate full context, distinguishing literal from figurative language.\n",
        "    - Account for common expressions vs actual threats\n",
        "    - Multiple categories can be true simultaneously.\n",
        "    - Assess intent and potential harm, considering audience sensitivity.\n",
        "    - When in doubt, analyze the intent and potential harm\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a content safety analyzer. Analyze the text and respond only with the requested JSON format.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=100,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        result = json.loads(response.choices[0].message.content)\n",
        "\n",
        "        column_mapping = {\n",
        "            \"Obscenity\": \"Obscenity and Profanity, including language, gestures, and explicitly gory, graphic or repulsive content intended to shock and disgust\",\n",
        "            \"Death\": \"Death, Injury or Military Conflict\",\n",
        "            \"Arms\": \"Arms and Ammunition\",\n",
        "            \"SensitiveIssues\": \"Debated Sensitive Social Issues\",\n",
        "            \"HateSpeech\": \"Hate Speech & Acts of aggression\",\n",
        "            \"Adult\": \"Adult& Explicit Sexual Content\",\n",
        "            \"Crime\": \"Crime & Harmful acts to individuals and Society and Human Right Violation\",\n",
        "            \"Drugs\": \"Illegal Drugs/Tobacco/e-cigarettes/Vaping/Alcohol\"\n",
        "        }\n",
        "\n",
        "        full_result = {column_mapping[k]: v for k, v in result.items()}\n",
        "        is_safe = not any(full_result.values())\n",
        "        full_result[\"Safe\"] = is_safe\n",
        "\n",
        "        return full_result\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON decode error: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_dataset(file_path, start_row=0, end_row=9610):\n",
        "\n",
        "    # Define directory for results in Google Drive\n",
        "    save_path = '/content/drive/MyDrive/content_analysis_results'\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    client = setup_openai()\n",
        "\n",
        "    # Read the dataset\n",
        "    df = pd.read_excel(file_path, nrows=end_row)\n",
        "\n",
        "    # Initialize required columns in the DataFrame if they are not already present\n",
        "    columns = [\n",
        "        \"TEXT\",\n",
        "        \"Obscenity and Profanity, including language, gestures, and explicitly gory, graphic or repulsive content intended to shock and disgust\",\n",
        "        \"Safe\",\n",
        "        \"Death, Injury or Military Conflict\",\n",
        "        \"Arms and Ammunition\",\n",
        "        \"Debated Sensitive Social Issues\",\n",
        "        \"Hate Speech & Acts of aggression\",\n",
        "        \"Adult& Explicit Sexual Content\",\n",
        "        \"Crime & Harmful acts to individuals and Society and Human Right Violation\",\n",
        "        \"Illegal Drugs/Tobacco/e-cigarettes/Vaping/Alcohol\"\n",
        "    ]\n",
        "\n",
        "    # Initialize columns\n",
        "    for column in columns[1:]:\n",
        "        if column not in df.columns:\n",
        "            df[column] = pd.Series(dtype='boolean')\n",
        "        else:\n",
        "            df[column] = df[column].astype('boolean')\n",
        "\n",
        "    total_rows = len(df)\n",
        "\n",
        "    # Create progress bar with description that will update\n",
        "    pbar = tqdm(total=total_rows, dynamic_ncols=True, position=0, leave=True)\n",
        "\n",
        "    for index, row in enumerate(df.iterrows()):\n",
        "        try:\n",
        "            # Update progress bar description with current text\n",
        "            text = row[1]['TEXT'][:100] + \"...\" if len(row[1]['TEXT']) > 100 else row[1]['TEXT']\n",
        "            pbar.set_description(f\"Processing [{index + 1}/{total_rows}]: {text}\")\n",
        "\n",
        "            result = analyze_text(client, row[1]['TEXT'])\n",
        "\n",
        "            if result:\n",
        "                for column in columns[1:]:\n",
        "                    df.at[index, column] = result[column]\n",
        "\n",
        "            # Save intermediate results every 50 rows to Google Drive\n",
        "            if (index + 1) % 500 == 0:\n",
        "                intermediate_file = f\"{save_path}/intermediate_results.xlsx\"\n",
        "                df.to_excel(intermediate_file, index=False)\n",
        "\n",
        "            pbar.update(1)\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing row {index}: {e}\")\n",
        "\n",
        "    # Close progress bar\n",
        "    pbar.close()\n",
        "\n",
        "    # Save final output locally\n",
        "    output_file = f\"{save_path}/final_output.xlsx\"\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"\\nFinal results saved to {output_file}\")\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/modified_prod_silver.xlsx\"\n",
        "    processed_df = process_dataset(\n",
        "        file_path=input_file,\n",
        "        start_row=0,     # Start from the beginning\n",
        "        end_row=9610     # Process up to row 9610\n",
        "    )\n"
      ]
    }
  ]
}